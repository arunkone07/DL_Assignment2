{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8008008,"sourceType":"datasetVersion","datasetId":4716602}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport wandb\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms.v2 as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-04T19:19:11.112086Z","iopub.execute_input":"2024-04-04T19:19:11.113091Z","iopub.status.idle":"2024-04-04T19:19:21.560652Z","shell.execute_reply.started":"2024-04-04T19:19:11.113028Z","shell.execute_reply":"2024-04-04T19:19:21.558959Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"wandb.login()\n# 3dc8367198d0460ba99efb94e713de7e299e685d","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:19:21.563286Z","iopub.execute_input":"2024-04-04T19:19:21.564187Z","iopub.status.idle":"2024-04-04T19:19:29.416458Z","shell.execute_reply.started":"2024-04-04T19:19:21.564130Z","shell.execute_reply":"2024-04-04T19:19:29.415170Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'random', \n    'metric': {\n      'name': 'accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'kernel_size':{\n            'values': [[3,3,3,3,3], [3,5,5,7,7], [7,7,5,5,3], [3,5,7,9,11] ]\n        },\n        'dropout': {\n            'values': [0, 0.2, 0.4]\n        },\n        'lr': {\n            'values': [0.0003, 0.001, 0.003]\n        },\n        'activation': {\n            'values': ['relu', 'gelu', 'silu', 'mish']\n        },\n        'optimizer': {\n            'values': ['adam', 'nadam']\n        },\n        'batch_norm':{\n            'values': ['true','false']\n        },\n        'filt_org':{\n            'values': [[32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512]]\n        },\n        'data_aug': {\n            'values': ['true','false']\n        },\n        'batch_size': {\n            'values': [32, 64, 128]\n        },\n        'num_dense':{\n            'values': [64, 128, 256]\n        }\n    }\n}\n\noptimizers = {\n    'adam': optim.Adam,\n    'nadam': optim.NAdam\n}\n\n\nsweep_id = wandb.sweep(sweep=sweep_config, project='Ass_2')","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:19:31.352636Z","iopub.execute_input":"2024-04-04T19:19:31.353366Z","iopub.status.idle":"2024-04-04T19:19:31.661783Z","shell.execute_reply.started":"2024-04-04T19:19:31.353321Z","shell.execute_reply":"2024-04-04T19:19:31.660805Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Create sweep with ID: 00v3pk4s\nSweep URL: https://wandb.ai/arun_cs23m017/Ass_2/sweeps/00v3pk4s\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = 10\nimg_size = 256\n\n# Define the CNN architecture\nclass CNN(nn.Module):\n    def __init__(self, config, num_classes=10):\n        super(CNN, self).__init__()\n        self.config = config\n        self.img_size = 256\n        self.in_channels = 3\n        self.num_classes = 10\n        self.num_epochs = 10\n        self.learning_rate = self.config.lr\n        self.batch_size = self.config.batch_size\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0).to(device)\n        self.bn1d = nn.BatchNorm1d(self.config.num_dense).to(device)\n        self.dropout = nn.Dropout(p=self.config.dropout).to(device)\n        self.fc2 = nn.Linear(self.config.num_dense, num_classes).to(device)\n\n        # Load and transform the data\n        transform = transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,), (0.5,))\n        ])\n        \n        transform_aug = transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.RandomRotation(degrees=30),\n            transforms.RandomVerticalFlip(),\n            transforms.ColorJitter(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,), (0.5,))\n        ])\n\n        if(self.config.data_aug == 'true'):\n            self.train_dataset = torchvision.datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/train', transform=transform_aug)\n#             self.test_dataset = torchvision.datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/val', transform=transform_aug)\n        else:\n            self.train_dataset = torchvision.datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/train', transform=transform)\n        \n        self.test_dataset = torchvision.datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/val', transform=transform) \n        self.train_dataset, self.val_dataset = torch.utils.data.random_split(self.train_dataset, [8000, 1999])\n        self.train_loader = torch.utils.data.DataLoader(dataset=self.train_dataset, batch_size=self.batch_size, shuffle=True)\n        self.val_loader = torch.utils.data.DataLoader(dataset=self.val_dataset, batch_size=self.batch_size, shuffle=True)\n        self.test_loader = torch.utils.data.DataLoader(dataset=self.test_dataset, batch_size=self.batch_size, shuffle=True)\n        \n        if self.config.activation == 'relu':\n            self.activation = F.relu\n        elif self.config.activation == 'gelu':\n            self.activation = F.gelu\n        elif self.config.activation == 'silu':\n            self.activation = F.silu\n        else:\n            self.activation = F.mish\n      \n    def forward(self, x):\n        self.in_channels = 3\n        self.wh = img_size\n        self.padding = 0\n        self.stride = 1\n        \n        for i in range(5):\n            self.conv = nn.Conv2d(self.in_channels, self.config.filt_org[i], self.config.kernel_size[i], self.stride, self.padding).to(device)\n            self.bn2d = nn.BatchNorm2d(self.config.filt_org[i]).to(device)\n            x = self.conv(x)\n            x = self.activation(x)\n            if(self.config.batch_norm == 'true'):\n                x = self.bn2d(x)\n            x = self.pool(x)\n            self.wh = (((self.wh - self.config.kernel_size[i] + 2*self.padding) // self.stride) + 1) // 2\n            x_shape = self.config.filt_org[i] * self.wh * self.wh\n            self.in_channels = self.config.filt_org[i]\n        x = x.view(-1,x_shape)\n        self.fc1 = nn.Linear(x_shape, self.config.num_dense).to(device)\n        x = self.activation(self.fc1(x))\n        if(self.config.batch_norm == 'true'):\n            x = self.bn1d(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n    \n    def accuracy(self, model, criterion, loader):\n        correct = 0\n        total = 0\n        total_loss = 0\n        with torch.no_grad():\n            for data in loader:\n                images, labels = data[0].to(device), data[1].to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                loss = criterion(outputs, labels)\n                total_loss += loss.item() * self.batch_size\n\n        acc = correct / total\n        total_loss /= total\n        return acc, total_loss\n    \n    def train(self, model, criterion, optimizer):\n        total_step = len(self.train_loader)\n        for epoch in range(self.num_epochs):\n            print(epoch)\n            correct = 0\n            train_loss = 0\n            for i, (images, labels) in enumerate(self.train_loader):\n                images, labels = images.to(device), labels.to(device)\n                # Forward pass \n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                correct += (predicted == labels).sum().item()\n                \n                loss = criterion(outputs, labels)\n\n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n                if (i+1)%10 == 0:\n                    print('Epoch [{}/{}], Step [{}/{}], Avg Loss: {:.4f}'.format(epoch+1, self.num_epochs, i+1, total_step, train_loss/(i+1)))\n            train_acc = correct / (total_step * self.batch_size) \n            train_loss /= total_step\n            val_acc, val_loss = self.accuracy(model, criterion, self.val_loader)\n            print(train_acc, train_loss,\n                  val_acc, val_loss, \"\\n\")\n            wandb.log({'train_accuracy': train_acc})\n            wandb.log({'train_loss': train_loss})\n            wandb.log({'val_accuracy': val_acc})\n            wandb.log({'val_loss': val_loss})","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:19:34.572465Z","iopub.execute_input":"2024-04-04T19:19:34.573738Z","iopub.status.idle":"2024-04-04T19:19:34.617501Z","shell.execute_reply.started":"2024-04-04T19:19:34.573680Z","shell.execute_reply":"2024-04-04T19:19:34.615962Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def main():\n    with wandb.init() as run:\n        model = CNN(wandb.config, num_classes).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optimizers[wandb.config.optimizer](model.parameters(), lr=wandb.config.lr)\n        model.train(model, criterion, optimizer)\n        print(model.accuracy(model, criterion, model.test_loader))\n\nwandb.agent(sweep_id, function=main, count=10) # calls main function for count number of times.\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:19:40.194692Z","iopub.execute_input":"2024-04-04T19:19:40.195742Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xzqgrih1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: true\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: false\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilt_org: [128, 128, 64, 64, 32]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 5, 7, 9, 11]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dense: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m017\u001b[0m (\u001b[33marun_cs23m017\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_191942-xzqgrih1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arun_cs23m017/Ass_2/runs/xzqgrih1' target=\"_blank\">wise-sweep-1</a></strong> to <a href='https://wandb.ai/arun_cs23m017/Ass_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/arun_cs23m017/Ass_2/sweeps/00v3pk4s' target=\"_blank\">https://wandb.ai/arun_cs23m017/Ass_2/sweeps/00v3pk4s</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arun_cs23m017/Ass_2' target=\"_blank\">https://wandb.ai/arun_cs23m017/Ass_2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/arun_cs23m017/Ass_2/sweeps/00v3pk4s' target=\"_blank\">https://wandb.ai/arun_cs23m017/Ass_2/sweeps/00v3pk4s</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arun_cs23m017/Ass_2/runs/xzqgrih1' target=\"_blank\">https://wandb.ai/arun_cs23m017/Ass_2/runs/xzqgrih1</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n  warnings.warn(\n","output_type":"stream"}]}]}