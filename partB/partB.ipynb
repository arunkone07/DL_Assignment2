{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# for downloading the data and unziping it\n",
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n",
        "!unzip -q nature_12K.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFf3yuoavAVg",
        "outputId": "d376472a-c12a-4e76-a079-eb222e62d8a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-09 11:16:59--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.206.207, 173.194.74.207, 209.85.145.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.206.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip      100%[===================>]   3.55G   166MB/s    in 27s     \n",
            "\n",
            "2024-04-09 11:17:26 (135 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb argparse"
      ],
      "metadata": {
        "id": "dWIP2kE-VR_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "img_size = 256\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "def cmd_parser():\n",
        "  args = argparse.ArgumentParser()\n",
        "  args.add_argument(\"--wandb_project\", \"-wp\", default=\"Assignment2\")\n",
        "  args.add_argument(\"--wandb_entity\", \"-we\", default=\"Assignment2\")\n",
        "  args.add_argument(\"--batch_norm\", \"-bn\", default=\"true\", choices=[\"true\", \"false\"])\n",
        "  args.add_argument(\"--batch_size\",\"-b\", type=int, default=32)\n",
        "  args.add_argument(\"--data_aug\", \"-da\", default=\"true\", choices=[\"true\", \"false\"])\n",
        "  args.add_argument(\"--dropout\", \"-dp\", default=0, type=float)\n",
        "  args.add_argument(\"--filt_org\", \"-fo\", default=\"double\", choices=[\"equal\", \"double\", \"half\"])\n",
        "  args.add_argument(\"--kernel_size\", \"-ks\", default=[3,3,3,3,3])\n",
        "  args.add_argument(\"--num_dense\", \"-nd\", default=64, type=int)\n",
        "  args.add_argument(\"--num_filters\",\"-nf\", default=128, type=int)\n",
        "  args.add_argument(\"--optimizer\",\"-o\", default= \"adam\", choices=[\"adam\",\"nadam\"])\n",
        "  args.add_argument(\"--learning_rate\",\"-lr\", default=0.003, type=float)\n",
        "  args.add_argument(\"--activation\", \"-a\", default=\"mish\", choices=[\"relu\",\"gelu\",\"silu\",\"mish\"])\n",
        "  args.add_argument(\"--strategy\", \"-s\", default=1, type=int)\n",
        "  return args.parse_args()\n",
        "\n",
        "args = cmd_parser()\n",
        "\n",
        "# Load and transform the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_aug = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset=None\n",
        "if(args.data_aug == 'true'):\n",
        "    train_dataset = torchvision.datasets.ImageFolder(root='/content/inaturalist_12K/train', transform=transform_aug)\n",
        "else:\n",
        "    train_dataset = torchvision.datasets.ImageFolder(root='/content/inaturalist_12K/train', transform=transform)\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [8000, 1999])\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='/content/inaturalist_12K/val', transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "def resnet_1():\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "def resnet_2(k):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    params = list(model.parameters())\n",
        "    for param in params[:k]:\n",
        "        param.requires_grad = False #freezing\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def resnet_3(num_dense):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = False\n",
        "    model.fc = nn.Sequential(\n",
        "      nn.Linear(model.fc.in_features,num_dense),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(num_dense, num_classes)\n",
        "    )\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "def accuracy(model, criterion, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loss += criterion(outputs, labels).item() * labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    loss /= total\n",
        "    return accuracy, loss\n",
        "\n",
        "def train(model, criterion, optimizer):\n",
        "    total_step = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            if (i+1)%10 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Avg Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, train_loss/(i+1)))\n",
        "\n",
        "        train_loss /= total_step\n",
        "        train_acc = correct / (total_step * args.batch_size)\n",
        "\n",
        "        val_acc, val_loss = accuracy(model, criterion, val_loader)\n",
        "\n",
        "        print(\"Train:\\nAccuracy:\", train_acc, \"Loss:\", train_loss)\n",
        "        print(\"Validation:\\nAccuracy:\", val_acc, \"Loss:\", val_loss, \"\\n\")\n",
        "\n",
        "optimizers = {\n",
        "        'adam': optim.Adam,\n",
        "        'nadam': optim.NAdam\n",
        "}\n",
        "\n",
        "wandb.init()\n",
        "bn = 0\n",
        "aug = 0\n",
        "org = 1\n",
        "ks = \"\"\n",
        "\n",
        "if(args.batch_norm == 'true'):\n",
        "    bn = 1\n",
        "\n",
        "if(args.data_aug == 'true'):\n",
        "    aug = 1\n",
        "\n",
        "if(args.filt_org == 'double'):\n",
        "    org = 2\n",
        "elif(args.filt_org == 'half'):\n",
        "    org = 0.5\n",
        "\n",
        "for i in range(0,5,2):\n",
        "    ks += str(args.kernel_size[i])\n",
        "\n",
        "wandb.run.name =  (args.activation + \"-bn_\"+str(bn) + \"-aug_\"+str(aug) + \"-drop_\"+str(args.dropout) +\n",
        "                    \"-bs_\"+str(args.batch_size) +\"-lr_\"+str(args.learning_rate) + \"-filt_\"+str(args.num_filters) +\n",
        "                    \"-org_\"+str(org) + \"-ks_\"+ks + \"-fc_\"+str(args.num_dense) + \"-\"+args.optimizer + \"-strategy_\"+str(args.strategy))\n",
        "model = None\n",
        "if(args.strategy == 1):\n",
        "    model = resnet_1()\n",
        "elif(args.strategy == 2):\n",
        "    model = resnet_2(10)\n",
        "else:\n",
        "    model = resnet_3(256)\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optimizers[args.optimizer](model.parameters(), lr=args.learning_rate)\n",
        "train(model, criterion, optimizer)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-9pcXROUZks",
        "outputId": "a71d2551-ce64-423d-bfbc-5ea86bb54ea5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py"
      ],
      "metadata": {
        "id": "UyxkGvlTu3Wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e95deac-152b-4d95-9f66-7f13b17e974d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m017\u001b[0m (\u001b[33marun_cs23m017\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240409_090437-s27nfkl4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mupbeat-dawn-15\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized/runs/s27nfkl4\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:01<00:00, 101MB/s]\n",
            "Epoch [1/10], Step [10/250], Avg Loss: 2.6270\n",
            "Epoch [1/10], Step [20/250], Avg Loss: 2.3047\n",
            "Epoch [1/10], Step [30/250], Avg Loss: 2.1461\n",
            "Epoch [1/10], Step [40/250], Avg Loss: 2.0112\n",
            "Epoch [1/10], Step [50/250], Avg Loss: 1.9025\n",
            "Epoch [1/10], Step [60/250], Avg Loss: 1.8248\n",
            "Epoch [1/10], Step [70/250], Avg Loss: 1.7670\n",
            "Epoch [1/10], Step [80/250], Avg Loss: 1.7096\n",
            "Epoch [1/10], Step [90/250], Avg Loss: 1.6634\n",
            "Epoch [1/10], Step [100/250], Avg Loss: 1.6387\n",
            "Epoch [1/10], Step [110/250], Avg Loss: 1.5893\n",
            "Epoch [1/10], Step [120/250], Avg Loss: 1.5632\n",
            "Epoch [1/10], Step [130/250], Avg Loss: 1.5318\n",
            "Epoch [1/10], Step [140/250], Avg Loss: 1.5174\n",
            "Epoch [1/10], Step [150/250], Avg Loss: 1.5011\n",
            "Epoch [1/10], Step [160/250], Avg Loss: 1.5020\n",
            "Epoch [1/10], Step [170/250], Avg Loss: 1.4946\n",
            "Epoch [1/10], Step [180/250], Avg Loss: 1.4793\n",
            "Epoch [1/10], Step [190/250], Avg Loss: 1.4653\n",
            "Epoch [1/10], Step [200/250], Avg Loss: 1.4576\n",
            "Epoch [1/10], Step [210/250], Avg Loss: 1.4474\n",
            "Epoch [1/10], Step [220/250], Avg Loss: 1.4409\n",
            "Epoch [1/10], Step [230/250], Avg Loss: 1.4331\n",
            "Epoch [1/10], Step [240/250], Avg Loss: 1.4231\n",
            "Epoch [1/10], Step [250/250], Avg Loss: 1.4142\n",
            "Train:\n",
            "Accuracy: 0.538625 Loss: 1.414222034215927\n",
            "Validation:\n",
            "Accuracy: 0.5472736368184092 Loss: 1.3615018493715318 \n",
            "\n",
            "Epoch [2/10], Step [10/250], Avg Loss: 1.1548\n",
            "Epoch [2/10], Step [20/250], Avg Loss: 1.1728\n",
            "Epoch [2/10], Step [30/250], Avg Loss: 1.1522\n",
            "Epoch [2/10], Step [40/250], Avg Loss: 1.1681\n",
            "Epoch [2/10], Step [50/250], Avg Loss: 1.1671\n",
            "Epoch [2/10], Step [60/250], Avg Loss: 1.1673\n",
            "Epoch [2/10], Step [70/250], Avg Loss: 1.1851\n",
            "Epoch [2/10], Step [80/250], Avg Loss: 1.1956\n",
            "Epoch [2/10], Step [90/250], Avg Loss: 1.2173\n",
            "Epoch [2/10], Step [100/250], Avg Loss: 1.2266\n",
            "Epoch [2/10], Step [110/250], Avg Loss: 1.2249\n",
            "Epoch [2/10], Step [120/250], Avg Loss: 1.2294\n",
            "Epoch [2/10], Step [130/250], Avg Loss: 1.2199\n",
            "Epoch [2/10], Step [140/250], Avg Loss: 1.2195\n",
            "Epoch [2/10], Step [150/250], Avg Loss: 1.2102\n",
            "Epoch [2/10], Step [160/250], Avg Loss: 1.2041\n",
            "Epoch [2/10], Step [170/250], Avg Loss: 1.2010\n",
            "Epoch [2/10], Step [180/250], Avg Loss: 1.2162\n",
            "Epoch [2/10], Step [190/250], Avg Loss: 1.2169\n",
            "Epoch [2/10], Step [200/250], Avg Loss: 1.2183\n",
            "Epoch [2/10], Step [210/250], Avg Loss: 1.2200\n",
            "Epoch [2/10], Step [220/250], Avg Loss: 1.2172\n",
            "Epoch [2/10], Step [230/250], Avg Loss: 1.2150\n",
            "Epoch [2/10], Step [240/250], Avg Loss: 1.2180\n",
            "Epoch [2/10], Step [250/250], Avg Loss: 1.2150\n",
            "Train:\n",
            "Accuracy: 0.6115 Loss: 1.2150032455921174\n",
            "Validation:\n",
            "Accuracy: 0.639319659829915 Loss: 1.1295851871572535 \n",
            "\n",
            "Epoch [3/10], Step [10/250], Avg Loss: 1.2960\n",
            "Epoch [3/10], Step [20/250], Avg Loss: 1.2339\n",
            "Epoch [3/10], Step [30/250], Avg Loss: 1.2827\n",
            "Epoch [3/10], Step [40/250], Avg Loss: 1.2864\n",
            "Epoch [3/10], Step [50/250], Avg Loss: 1.3158\n",
            "Epoch [3/10], Step [60/250], Avg Loss: 1.3051\n",
            "Epoch [3/10], Step [70/250], Avg Loss: 1.3029\n",
            "Epoch [3/10], Step [80/250], Avg Loss: 1.3040\n",
            "Epoch [3/10], Step [90/250], Avg Loss: 1.2963\n",
            "Epoch [3/10], Step [100/250], Avg Loss: 1.2913\n",
            "Epoch [3/10], Step [110/250], Avg Loss: 1.2678\n",
            "Epoch [3/10], Step [120/250], Avg Loss: 1.2612\n",
            "Epoch [3/10], Step [130/250], Avg Loss: 1.2542\n",
            "Epoch [3/10], Step [140/250], Avg Loss: 1.2419\n",
            "Epoch [3/10], Step [150/250], Avg Loss: 1.2397\n",
            "Epoch [3/10], Step [160/250], Avg Loss: 1.2271\n",
            "Epoch [3/10], Step [170/250], Avg Loss: 1.2166\n",
            "Epoch [3/10], Step [180/250], Avg Loss: 1.2134\n",
            "Epoch [3/10], Step [190/250], Avg Loss: 1.2105\n",
            "Epoch [3/10], Step [200/250], Avg Loss: 1.2152\n",
            "Epoch [3/10], Step [210/250], Avg Loss: 1.2141\n",
            "Epoch [3/10], Step [220/250], Avg Loss: 1.2235\n",
            "Epoch [3/10], Step [230/250], Avg Loss: 1.2206\n",
            "Epoch [3/10], Step [240/250], Avg Loss: 1.2170\n",
            "Epoch [3/10], Step [250/250], Avg Loss: 1.2115\n",
            "Train:\n",
            "Accuracy: 0.6185 Loss: 1.2115322015285492\n",
            "Validation:\n",
            "Accuracy: 0.591295647823912 Loss: 1.302237008678251 \n",
            "\n",
            "Epoch [4/10], Step [10/250], Avg Loss: 0.9567\n",
            "Epoch [4/10], Step [20/250], Avg Loss: 1.0067\n",
            "Epoch [4/10], Step [30/250], Avg Loss: 1.0797\n",
            "Epoch [4/10], Step [40/250], Avg Loss: 1.1608\n",
            "Epoch [4/10], Step [50/250], Avg Loss: 1.1952\n",
            "Epoch [4/10], Step [60/250], Avg Loss: 1.1713\n",
            "Epoch [4/10], Step [70/250], Avg Loss: 1.1837\n",
            "Epoch [4/10], Step [80/250], Avg Loss: 1.1884\n",
            "Epoch [4/10], Step [90/250], Avg Loss: 1.1801\n",
            "Epoch [4/10], Step [100/250], Avg Loss: 1.1854\n",
            "Epoch [4/10], Step [110/250], Avg Loss: 1.1934\n",
            "Epoch [4/10], Step [120/250], Avg Loss: 1.1894\n",
            "Epoch [4/10], Step [130/250], Avg Loss: 1.1844\n",
            "Epoch [4/10], Step [140/250], Avg Loss: 1.1801\n",
            "Epoch [4/10], Step [150/250], Avg Loss: 1.1825\n",
            "Epoch [4/10], Step [160/250], Avg Loss: 1.1801\n",
            "Epoch [4/10], Step [170/250], Avg Loss: 1.1755\n",
            "Epoch [4/10], Step [180/250], Avg Loss: 1.1806\n",
            "Epoch [4/10], Step [190/250], Avg Loss: 1.1784\n",
            "Epoch [4/10], Step [200/250], Avg Loss: 1.1830\n",
            "Epoch [4/10], Step [210/250], Avg Loss: 1.1768\n",
            "Epoch [4/10], Step [220/250], Avg Loss: 1.1808\n",
            "Epoch [4/10], Step [230/250], Avg Loss: 1.1923\n",
            "Epoch [4/10], Step [240/250], Avg Loss: 1.1951\n",
            "Epoch [4/10], Step [250/250], Avg Loss: 1.1992\n",
            "Train:\n",
            "Accuracy: 0.61925 Loss: 1.199178642630577\n",
            "Validation:\n",
            "Accuracy: 0.5507753876938469 Loss: 1.5584678423291387 \n",
            "\n",
            "Epoch [5/10], Step [10/250], Avg Loss: 1.2474\n",
            "Epoch [5/10], Step [20/250], Avg Loss: 1.2228\n",
            "Epoch [5/10], Step [30/250], Avg Loss: 1.2109\n",
            "Epoch [5/10], Step [40/250], Avg Loss: 1.2431\n",
            "Epoch [5/10], Step [50/250], Avg Loss: 1.3128\n",
            "Epoch [5/10], Step [60/250], Avg Loss: 1.3199\n",
            "Epoch [5/10], Step [70/250], Avg Loss: 1.3109\n",
            "Epoch [5/10], Step [80/250], Avg Loss: 1.2996\n",
            "Epoch [5/10], Step [90/250], Avg Loss: 1.2851\n",
            "Epoch [5/10], Step [100/250], Avg Loss: 1.2752\n",
            "Epoch [5/10], Step [110/250], Avg Loss: 1.2597\n",
            "Epoch [5/10], Step [120/250], Avg Loss: 1.2523\n",
            "Epoch [5/10], Step [130/250], Avg Loss: 1.2436\n",
            "Epoch [5/10], Step [140/250], Avg Loss: 1.2343\n",
            "Epoch [5/10], Step [150/250], Avg Loss: 1.2300\n",
            "Epoch [5/10], Step [160/250], Avg Loss: 1.2356\n",
            "Epoch [5/10], Step [170/250], Avg Loss: 1.2423\n",
            "Epoch [5/10], Step [180/250], Avg Loss: 1.2419\n",
            "Epoch [5/10], Step [190/250], Avg Loss: 1.2300\n",
            "Epoch [5/10], Step [200/250], Avg Loss: 1.2267\n",
            "Epoch [5/10], Step [210/250], Avg Loss: 1.2163\n",
            "Epoch [5/10], Step [220/250], Avg Loss: 1.2098\n",
            "Epoch [5/10], Step [230/250], Avg Loss: 1.2056\n",
            "Epoch [5/10], Step [240/250], Avg Loss: 1.2055\n",
            "Epoch [5/10], Step [250/250], Avg Loss: 1.2034\n",
            "Train:\n",
            "Accuracy: 0.625375 Loss: 1.2034154152870178\n",
            "Validation:\n",
            "Accuracy: 0.6308154077038519 Loss: 1.1583308445447202 \n",
            "\n",
            "Epoch [6/10], Step [10/250], Avg Loss: 1.0725\n",
            "Epoch [6/10], Step [20/250], Avg Loss: 1.2075\n",
            "Epoch [6/10], Step [30/250], Avg Loss: 1.1960\n",
            "Epoch [6/10], Step [40/250], Avg Loss: 1.2153\n",
            "Epoch [6/10], Step [50/250], Avg Loss: 1.2066\n",
            "Epoch [6/10], Step [60/250], Avg Loss: 1.2102\n",
            "Epoch [6/10], Step [70/250], Avg Loss: 1.1828\n",
            "Epoch [6/10], Step [80/250], Avg Loss: 1.1858\n",
            "Epoch [6/10], Step [90/250], Avg Loss: 1.1865\n",
            "Epoch [6/10], Step [100/250], Avg Loss: 1.1782\n",
            "Epoch [6/10], Step [110/250], Avg Loss: 1.1606\n",
            "Epoch [6/10], Step [120/250], Avg Loss: 1.1476\n",
            "Epoch [6/10], Step [130/250], Avg Loss: 1.1389\n",
            "Epoch [6/10], Step [140/250], Avg Loss: 1.1359\n",
            "Epoch [6/10], Step [150/250], Avg Loss: 1.1475\n",
            "Epoch [6/10], Step [160/250], Avg Loss: 1.1471\n",
            "Epoch [6/10], Step [170/250], Avg Loss: 1.1476\n",
            "Epoch [6/10], Step [180/250], Avg Loss: 1.1457\n",
            "Epoch [6/10], Step [190/250], Avg Loss: 1.1522\n",
            "Epoch [6/10], Step [200/250], Avg Loss: 1.1686\n",
            "Epoch [6/10], Step [210/250], Avg Loss: 1.1745\n",
            "Epoch [6/10], Step [220/250], Avg Loss: 1.1718\n",
            "Epoch [6/10], Step [230/250], Avg Loss: 1.1640\n",
            "Epoch [6/10], Step [240/250], Avg Loss: 1.1648\n",
            "Epoch [6/10], Step [250/250], Avg Loss: 1.1661\n",
            "Train:\n",
            "Accuracy: 0.634 Loss: 1.166099445104599\n",
            "Validation:\n",
            "Accuracy: 0.5887943971985993 Loss: 1.3887979977723657 \n",
            "\n",
            "Epoch [7/10], Step [10/250], Avg Loss: 1.2456\n",
            "Epoch [7/10], Step [20/250], Avg Loss: 1.1858\n",
            "Epoch [7/10], Step [30/250], Avg Loss: 1.1624\n",
            "Epoch [7/10], Step [40/250], Avg Loss: 1.0892\n",
            "Epoch [7/10], Step [50/250], Avg Loss: 1.1030\n",
            "Epoch [7/10], Step [60/250], Avg Loss: 1.0949\n",
            "Epoch [7/10], Step [70/250], Avg Loss: 1.1070\n",
            "Epoch [7/10], Step [80/250], Avg Loss: 1.0850\n",
            "Epoch [7/10], Step [90/250], Avg Loss: 1.0663\n",
            "Epoch [7/10], Step [100/250], Avg Loss: 1.0802\n",
            "Epoch [7/10], Step [110/250], Avg Loss: 1.0843\n",
            "Epoch [7/10], Step [120/250], Avg Loss: 1.0811\n",
            "Epoch [7/10], Step [130/250], Avg Loss: 1.0884\n",
            "Epoch [7/10], Step [140/250], Avg Loss: 1.1010\n",
            "Epoch [7/10], Step [150/250], Avg Loss: 1.0996\n",
            "Epoch [7/10], Step [160/250], Avg Loss: 1.1051\n",
            "Epoch [7/10], Step [170/250], Avg Loss: 1.0981\n",
            "Epoch [7/10], Step [180/250], Avg Loss: 1.0993\n",
            "Epoch [7/10], Step [190/250], Avg Loss: 1.0959\n",
            "Epoch [7/10], Step [200/250], Avg Loss: 1.0976\n",
            "Epoch [7/10], Step [210/250], Avg Loss: 1.0968\n",
            "Epoch [7/10], Step [220/250], Avg Loss: 1.0917\n",
            "Epoch [7/10], Step [230/250], Avg Loss: 1.0963\n",
            "Epoch [7/10], Step [240/250], Avg Loss: 1.0951\n",
            "Epoch [7/10], Step [250/250], Avg Loss: 1.1037\n",
            "Train:\n",
            "Accuracy: 0.650375 Loss: 1.1036535738706588\n",
            "Validation:\n",
            "Accuracy: 0.5707853926963482 Loss: 1.4210363232296308 \n",
            "\n",
            "Epoch [8/10], Step [10/250], Avg Loss: 1.0565\n",
            "Epoch [8/10], Step [20/250], Avg Loss: 1.0012\n",
            "Epoch [8/10], Step [30/250], Avg Loss: 1.0886\n",
            "Epoch [8/10], Step [40/250], Avg Loss: 1.1181\n",
            "Epoch [8/10], Step [50/250], Avg Loss: 1.0929\n",
            "Epoch [8/10], Step [60/250], Avg Loss: 1.0945\n",
            "Epoch [8/10], Step [70/250], Avg Loss: 1.1109\n",
            "Epoch [8/10], Step [80/250], Avg Loss: 1.1274\n",
            "Epoch [8/10], Step [90/250], Avg Loss: 1.1081\n",
            "Epoch [8/10], Step [100/250], Avg Loss: 1.1184\n",
            "Epoch [8/10], Step [110/250], Avg Loss: 1.1174\n",
            "Epoch [8/10], Step [120/250], Avg Loss: 1.1283\n",
            "Epoch [8/10], Step [130/250], Avg Loss: 1.1258\n",
            "Epoch [8/10], Step [140/250], Avg Loss: 1.1452\n",
            "Epoch [8/10], Step [150/250], Avg Loss: 1.1389\n",
            "Epoch [8/10], Step [160/250], Avg Loss: 1.1446\n",
            "Epoch [8/10], Step [170/250], Avg Loss: 1.1498\n",
            "Epoch [8/10], Step [180/250], Avg Loss: 1.1534\n",
            "Epoch [8/10], Step [190/250], Avg Loss: 1.1583\n",
            "Epoch [8/10], Step [200/250], Avg Loss: 1.1583\n",
            "Epoch [8/10], Step [210/250], Avg Loss: 1.1520\n",
            "Epoch [8/10], Step [220/250], Avg Loss: 1.1588\n",
            "Epoch [8/10], Step [230/250], Avg Loss: 1.1582\n",
            "Epoch [8/10], Step [240/250], Avg Loss: 1.1523\n",
            "Epoch [8/10], Step [250/250], Avg Loss: 1.1562\n",
            "Train:\n",
            "Accuracy: 0.641875 Loss: 1.1562118005752564\n",
            "Validation:\n",
            "Accuracy: 0.6098049024512256 Loss: 1.316460762905323 \n",
            "\n",
            "Epoch [9/10], Step [10/250], Avg Loss: 1.1546\n",
            "Epoch [9/10], Step [20/250], Avg Loss: 1.0542\n",
            "Epoch [9/10], Step [30/250], Avg Loss: 1.0810\n",
            "Epoch [9/10], Step [40/250], Avg Loss: 1.0623\n",
            "Epoch [9/10], Step [50/250], Avg Loss: 1.0718\n",
            "Epoch [9/10], Step [60/250], Avg Loss: 1.0765\n",
            "Epoch [9/10], Step [70/250], Avg Loss: 1.0766\n",
            "Epoch [9/10], Step [80/250], Avg Loss: 1.0994\n",
            "Epoch [9/10], Step [90/250], Avg Loss: 1.0937\n",
            "Epoch [9/10], Step [100/250], Avg Loss: 1.1069\n",
            "Epoch [9/10], Step [110/250], Avg Loss: 1.0957\n",
            "Epoch [9/10], Step [120/250], Avg Loss: 1.0967\n",
            "Epoch [9/10], Step [130/250], Avg Loss: 1.0846\n",
            "Epoch [9/10], Step [140/250], Avg Loss: 1.0846\n",
            "Epoch [9/10], Step [150/250], Avg Loss: 1.0819\n",
            "Epoch [9/10], Step [160/250], Avg Loss: 1.0793\n",
            "Epoch [9/10], Step [170/250], Avg Loss: 1.0907\n",
            "Epoch [9/10], Step [180/250], Avg Loss: 1.0952\n",
            "Epoch [9/10], Step [190/250], Avg Loss: 1.1068\n",
            "Epoch [9/10], Step [200/250], Avg Loss: 1.1043\n",
            "Epoch [9/10], Step [210/250], Avg Loss: 1.1002\n",
            "Epoch [9/10], Step [220/250], Avg Loss: 1.0922\n",
            "Epoch [9/10], Step [230/250], Avg Loss: 1.0928\n",
            "Epoch [9/10], Step [240/250], Avg Loss: 1.0941\n",
            "Epoch [9/10], Step [250/250], Avg Loss: 1.1009\n",
            "Train:\n",
            "Accuracy: 0.65 Loss: 1.1009429899454117\n",
            "Validation:\n",
            "Accuracy: 0.624312156078039 Loss: 1.2236944234866152 \n",
            "\n",
            "Epoch [10/10], Step [10/250], Avg Loss: 1.0433\n",
            "Epoch [10/10], Step [20/250], Avg Loss: 1.0021\n",
            "Epoch [10/10], Step [30/250], Avg Loss: 1.0538\n",
            "Epoch [10/10], Step [40/250], Avg Loss: 1.0630\n",
            "Epoch [10/10], Step [50/250], Avg Loss: 1.1084\n",
            "Epoch [10/10], Step [60/250], Avg Loss: 1.0940\n",
            "Epoch [10/10], Step [70/250], Avg Loss: 1.1003\n",
            "Epoch [10/10], Step [80/250], Avg Loss: 1.1149\n",
            "Epoch [10/10], Step [90/250], Avg Loss: 1.1139\n",
            "Epoch [10/10], Step [100/250], Avg Loss: 1.1194\n",
            "Epoch [10/10], Step [110/250], Avg Loss: 1.1317\n",
            "Epoch [10/10], Step [120/250], Avg Loss: 1.1209\n",
            "Epoch [10/10], Step [130/250], Avg Loss: 1.1240\n",
            "Epoch [10/10], Step [140/250], Avg Loss: 1.1168\n",
            "Epoch [10/10], Step [150/250], Avg Loss: 1.1041\n",
            "Epoch [10/10], Step [160/250], Avg Loss: 1.1053\n",
            "Epoch [10/10], Step [170/250], Avg Loss: 1.0968\n",
            "Epoch [10/10], Step [180/250], Avg Loss: 1.1066\n",
            "Epoch [10/10], Step [190/250], Avg Loss: 1.1035\n",
            "Epoch [10/10], Step [200/250], Avg Loss: 1.1095\n",
            "Epoch [10/10], Step [210/250], Avg Loss: 1.1162\n",
            "Epoch [10/10], Step [220/250], Avg Loss: 1.1163\n",
            "Epoch [10/10], Step [230/250], Avg Loss: 1.1129\n",
            "Epoch [10/10], Step [240/250], Avg Loss: 1.1136\n",
            "Epoch [10/10], Step [250/250], Avg Loss: 1.1119\n",
            "Train:\n",
            "Accuracy: 0.652625 Loss: 1.1119298235177995\n",
            "Validation:\n",
            "Accuracy: 0.5502751375687844 Loss: 1.6234700244149307 \n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mupbeat-dawn-15\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized/runs/s27nfkl4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240409_090437-s27nfkl4/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -s 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCY_GUOgkmK8",
        "outputId": "725de937-3ca5-4ac4-bd93-c350c60bb8f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m017\u001b[0m (\u001b[33marun_cs23m017\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240409_112920-goi9gewx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhelpful-donkey-17\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized/runs/goi9gewx\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch [1/10], Step [10/250], Avg Loss: 2.9057\n",
            "Epoch [1/10], Step [20/250], Avg Loss: 2.6330\n",
            "Epoch [1/10], Step [30/250], Avg Loss: 2.5751\n",
            "Epoch [1/10], Step [40/250], Avg Loss: 2.5233\n",
            "Epoch [1/10], Step [50/250], Avg Loss: 2.4789\n",
            "Epoch [1/10], Step [60/250], Avg Loss: 2.4486\n",
            "Epoch [1/10], Step [70/250], Avg Loss: 2.4303\n",
            "Epoch [1/10], Step [80/250], Avg Loss: 2.4142\n",
            "Epoch [1/10], Step [90/250], Avg Loss: 2.4019\n",
            "Epoch [1/10], Step [100/250], Avg Loss: 2.3919\n",
            "Epoch [1/10], Step [110/250], Avg Loss: 2.3773\n",
            "Epoch [1/10], Step [120/250], Avg Loss: 2.3684\n",
            "Epoch [1/10], Step [130/250], Avg Loss: 2.3559\n",
            "Epoch [1/10], Step [140/250], Avg Loss: 2.3466\n",
            "Epoch [1/10], Step [150/250], Avg Loss: 2.3382\n",
            "Epoch [1/10], Step [160/250], Avg Loss: 2.3311\n",
            "Epoch [1/10], Step [170/250], Avg Loss: 2.3245\n",
            "Epoch [1/10], Step [180/250], Avg Loss: 2.3167\n",
            "Epoch [1/10], Step [190/250], Avg Loss: 2.3072\n",
            "Epoch [1/10], Step [200/250], Avg Loss: 2.3012\n",
            "Epoch [1/10], Step [210/250], Avg Loss: 2.2920\n",
            "Epoch [1/10], Step [220/250], Avg Loss: 2.2895\n",
            "Epoch [1/10], Step [230/250], Avg Loss: 2.2841\n",
            "Epoch [1/10], Step [240/250], Avg Loss: 2.2801\n",
            "Epoch [1/10], Step [250/250], Avg Loss: 2.2758\n",
            "Train:\n",
            "Accuracy: 0.16675 Loss: 2.275763816356659\n",
            "Validation:\n",
            "Accuracy: 0.21510755377688845 Loss: 2.120723968568356 \n",
            "\n",
            "Epoch [2/10], Step [10/250], Avg Loss: 2.1486\n",
            "Epoch [2/10], Step [20/250], Avg Loss: 2.1276\n",
            "Epoch [2/10], Step [30/250], Avg Loss: 2.1298\n",
            "Epoch [2/10], Step [40/250], Avg Loss: 2.1268\n",
            "Epoch [2/10], Step [50/250], Avg Loss: 2.0978\n",
            "Epoch [2/10], Step [60/250], Avg Loss: 2.0938\n",
            "Epoch [2/10], Step [70/250], Avg Loss: 2.0985\n",
            "Epoch [2/10], Step [80/250], Avg Loss: 2.0923\n",
            "Epoch [2/10], Step [90/250], Avg Loss: 2.0904\n",
            "Epoch [2/10], Step [100/250], Avg Loss: 2.0976\n",
            "Epoch [2/10], Step [110/250], Avg Loss: 2.0962\n",
            "Epoch [2/10], Step [120/250], Avg Loss: 2.0950\n",
            "Epoch [2/10], Step [130/250], Avg Loss: 2.0913\n",
            "Epoch [2/10], Step [140/250], Avg Loss: 2.0949\n",
            "Epoch [2/10], Step [150/250], Avg Loss: 2.0943\n",
            "Epoch [2/10], Step [160/250], Avg Loss: 2.0945\n",
            "Epoch [2/10], Step [170/250], Avg Loss: 2.0948\n",
            "Epoch [2/10], Step [180/250], Avg Loss: 2.0945\n",
            "Epoch [2/10], Step [190/250], Avg Loss: 2.0928\n",
            "Epoch [2/10], Step [200/250], Avg Loss: 2.0908\n",
            "Epoch [2/10], Step [210/250], Avg Loss: 2.0886\n",
            "Epoch [2/10], Step [220/250], Avg Loss: 2.0874\n",
            "Epoch [2/10], Step [230/250], Avg Loss: 2.0849\n",
            "Epoch [2/10], Step [240/250], Avg Loss: 2.0842\n",
            "Epoch [2/10], Step [250/250], Avg Loss: 2.0854\n",
            "Train:\n",
            "Accuracy: 0.242125 Loss: 2.0853548374176025\n",
            "Validation:\n",
            "Accuracy: 0.2666333166583292 Loss: 2.0174766375220616 \n",
            "\n",
            "Epoch [3/10], Step [10/250], Avg Loss: 2.0518\n",
            "Epoch [3/10], Step [20/250], Avg Loss: 2.0139\n",
            "Epoch [3/10], Step [30/250], Avg Loss: 2.0200\n",
            "Epoch [3/10], Step [40/250], Avg Loss: 2.0311\n",
            "Epoch [3/10], Step [50/250], Avg Loss: 2.0561\n",
            "Epoch [3/10], Step [60/250], Avg Loss: 2.0593\n",
            "Epoch [3/10], Step [70/250], Avg Loss: 2.0440\n",
            "Epoch [3/10], Step [80/250], Avg Loss: 2.0422\n",
            "Epoch [3/10], Step [90/250], Avg Loss: 2.0374\n",
            "Epoch [3/10], Step [100/250], Avg Loss: 2.0433\n",
            "Epoch [3/10], Step [110/250], Avg Loss: 2.0415\n",
            "Epoch [3/10], Step [120/250], Avg Loss: 2.0372\n",
            "Epoch [3/10], Step [130/250], Avg Loss: 2.0339\n",
            "Epoch [3/10], Step [140/250], Avg Loss: 2.0272\n",
            "Epoch [3/10], Step [150/250], Avg Loss: 2.0196\n",
            "Epoch [3/10], Step [160/250], Avg Loss: 2.0227\n",
            "Epoch [3/10], Step [170/250], Avg Loss: 2.0209\n",
            "Epoch [3/10], Step [180/250], Avg Loss: 2.0229\n",
            "Epoch [3/10], Step [190/250], Avg Loss: 2.0278\n",
            "Epoch [3/10], Step [200/250], Avg Loss: 2.0253\n",
            "Epoch [3/10], Step [210/250], Avg Loss: 2.0229\n",
            "Epoch [3/10], Step [220/250], Avg Loss: 2.0247\n",
            "Epoch [3/10], Step [230/250], Avg Loss: 2.0267\n",
            "Epoch [3/10], Step [240/250], Avg Loss: 2.0227\n",
            "Epoch [3/10], Step [250/250], Avg Loss: 2.0220\n",
            "Train:\n",
            "Accuracy: 0.267125 Loss: 2.0220244593620302\n",
            "Validation:\n",
            "Accuracy: 0.3031515757878939 Loss: 1.9732434082889987 \n",
            "\n",
            "Epoch [4/10], Step [10/250], Avg Loss: 2.0451\n",
            "Epoch [4/10], Step [20/250], Avg Loss: 1.9938\n",
            "Epoch [4/10], Step [30/250], Avg Loss: 1.9858\n",
            "Epoch [4/10], Step [40/250], Avg Loss: 1.9744\n",
            "Epoch [4/10], Step [50/250], Avg Loss: 1.9765\n",
            "Epoch [4/10], Step [60/250], Avg Loss: 1.9692\n",
            "Epoch [4/10], Step [70/250], Avg Loss: 1.9716\n",
            "Epoch [4/10], Step [80/250], Avg Loss: 1.9846\n",
            "Epoch [4/10], Step [90/250], Avg Loss: 1.9822\n",
            "Epoch [4/10], Step [100/250], Avg Loss: 1.9846\n",
            "Epoch [4/10], Step [110/250], Avg Loss: 1.9767\n",
            "Epoch [4/10], Step [120/250], Avg Loss: 1.9787\n",
            "Epoch [4/10], Step [130/250], Avg Loss: 1.9797\n",
            "Epoch [4/10], Step [140/250], Avg Loss: 1.9738\n",
            "Epoch [4/10], Step [150/250], Avg Loss: 1.9747\n",
            "Epoch [4/10], Step [160/250], Avg Loss: 1.9789\n",
            "Epoch [4/10], Step [170/250], Avg Loss: 1.9788\n",
            "Epoch [4/10], Step [180/250], Avg Loss: 1.9760\n",
            "Epoch [4/10], Step [190/250], Avg Loss: 1.9761\n",
            "Epoch [4/10], Step [200/250], Avg Loss: 1.9754\n",
            "Epoch [4/10], Step [210/250], Avg Loss: 1.9774\n",
            "Epoch [4/10], Step [220/250], Avg Loss: 1.9765\n",
            "Epoch [4/10], Step [230/250], Avg Loss: 1.9747\n",
            "Epoch [4/10], Step [240/250], Avg Loss: 1.9755\n",
            "Epoch [4/10], Step [250/250], Avg Loss: 1.9746\n",
            "Train:\n",
            "Accuracy: 0.2865 Loss: 1.9745859088897706\n",
            "Validation:\n",
            "Accuracy: 0.3246623311655828 Loss: 1.9158730686993048 \n",
            "\n",
            "Epoch [5/10], Step [10/250], Avg Loss: 2.0431\n",
            "Epoch [5/10], Step [20/250], Avg Loss: 2.0349\n",
            "Epoch [5/10], Step [30/250], Avg Loss: 2.0010\n",
            "Epoch [5/10], Step [40/250], Avg Loss: 1.9861\n",
            "Epoch [5/10], Step [50/250], Avg Loss: 1.9813\n",
            "Epoch [5/10], Step [60/250], Avg Loss: 1.9739\n",
            "Epoch [5/10], Step [70/250], Avg Loss: 1.9619\n",
            "Epoch [5/10], Step [80/250], Avg Loss: 1.9699\n",
            "Epoch [5/10], Step [90/250], Avg Loss: 1.9642\n",
            "Epoch [5/10], Step [100/250], Avg Loss: 1.9621\n",
            "Epoch [5/10], Step [110/250], Avg Loss: 1.9585\n",
            "Epoch [5/10], Step [120/250], Avg Loss: 1.9563\n",
            "Epoch [5/10], Step [130/250], Avg Loss: 1.9493\n",
            "Epoch [5/10], Step [140/250], Avg Loss: 1.9492\n",
            "Epoch [5/10], Step [150/250], Avg Loss: 1.9488\n",
            "Epoch [5/10], Step [160/250], Avg Loss: 1.9480\n",
            "Epoch [5/10], Step [170/250], Avg Loss: 1.9503\n",
            "Epoch [5/10], Step [180/250], Avg Loss: 1.9532\n",
            "Epoch [5/10], Step [190/250], Avg Loss: 1.9533\n",
            "Epoch [5/10], Step [200/250], Avg Loss: 1.9524\n",
            "Epoch [5/10], Step [210/250], Avg Loss: 1.9530\n",
            "Epoch [5/10], Step [220/250], Avg Loss: 1.9518\n",
            "Epoch [5/10], Step [230/250], Avg Loss: 1.9478\n",
            "Epoch [5/10], Step [240/250], Avg Loss: 1.9459\n",
            "Epoch [5/10], Step [250/250], Avg Loss: 1.9437\n",
            "Train:\n",
            "Accuracy: 0.307875 Loss: 1.9437241616249084\n",
            "Validation:\n",
            "Accuracy: 0.32216108054027015 Loss: 1.8979880147245063 \n",
            "\n",
            "Epoch [6/10], Step [10/250], Avg Loss: 1.9041\n",
            "Epoch [6/10], Step [20/250], Avg Loss: 1.9256\n",
            "Epoch [6/10], Step [30/250], Avg Loss: 1.9478\n",
            "Epoch [6/10], Step [40/250], Avg Loss: 1.9328\n",
            "Epoch [6/10], Step [50/250], Avg Loss: 1.9236\n",
            "Epoch [6/10], Step [60/250], Avg Loss: 1.9250\n",
            "Epoch [6/10], Step [70/250], Avg Loss: 1.9345\n",
            "Epoch [6/10], Step [80/250], Avg Loss: 1.9230\n",
            "Epoch [6/10], Step [90/250], Avg Loss: 1.9223\n",
            "Epoch [6/10], Step [100/250], Avg Loss: 1.9192\n",
            "Epoch [6/10], Step [110/250], Avg Loss: 1.9183\n",
            "Epoch [6/10], Step [120/250], Avg Loss: 1.9183\n",
            "Epoch [6/10], Step [130/250], Avg Loss: 1.9240\n",
            "Epoch [6/10], Step [140/250], Avg Loss: 1.9207\n",
            "Epoch [6/10], Step [150/250], Avg Loss: 1.9153\n",
            "Epoch [6/10], Step [160/250], Avg Loss: 1.9121\n",
            "Epoch [6/10], Step [170/250], Avg Loss: 1.9165\n",
            "Epoch [6/10], Step [180/250], Avg Loss: 1.9150\n",
            "Epoch [6/10], Step [190/250], Avg Loss: 1.9181\n",
            "Epoch [6/10], Step [200/250], Avg Loss: 1.9144\n",
            "Epoch [6/10], Step [210/250], Avg Loss: 1.9134\n",
            "Epoch [6/10], Step [220/250], Avg Loss: 1.9124\n",
            "Epoch [6/10], Step [230/250], Avg Loss: 1.9107\n",
            "Epoch [6/10], Step [240/250], Avg Loss: 1.9130\n",
            "Epoch [6/10], Step [250/250], Avg Loss: 1.9098\n",
            "Train:\n",
            "Accuracy: 0.314125 Loss: 1.9097513971328735\n",
            "Validation:\n",
            "Accuracy: 0.3306653326663332 Loss: 1.8819333906469493 \n",
            "\n",
            "Epoch [7/10], Step [10/250], Avg Loss: 1.8411\n",
            "Epoch [7/10], Step [20/250], Avg Loss: 1.8416\n",
            "Epoch [7/10], Step [30/250], Avg Loss: 1.8671\n",
            "Epoch [7/10], Step [40/250], Avg Loss: 1.8563\n",
            "Epoch [7/10], Step [50/250], Avg Loss: 1.8435\n",
            "Epoch [7/10], Step [60/250], Avg Loss: 1.8662\n",
            "Epoch [7/10], Step [70/250], Avg Loss: 1.8652\n",
            "Epoch [7/10], Step [80/250], Avg Loss: 1.8635\n",
            "Epoch [7/10], Step [90/250], Avg Loss: 1.8685\n",
            "Epoch [7/10], Step [100/250], Avg Loss: 1.8755\n",
            "Epoch [7/10], Step [110/250], Avg Loss: 1.8683\n",
            "Epoch [7/10], Step [120/250], Avg Loss: 1.8728\n",
            "Epoch [7/10], Step [130/250], Avg Loss: 1.8717\n",
            "Epoch [7/10], Step [140/250], Avg Loss: 1.8741\n",
            "Epoch [7/10], Step [150/250], Avg Loss: 1.8729\n",
            "Epoch [7/10], Step [160/250], Avg Loss: 1.8764\n",
            "Epoch [7/10], Step [170/250], Avg Loss: 1.8793\n",
            "Epoch [7/10], Step [180/250], Avg Loss: 1.8822\n",
            "Epoch [7/10], Step [190/250], Avg Loss: 1.8811\n",
            "Epoch [7/10], Step [200/250], Avg Loss: 1.8839\n",
            "Epoch [7/10], Step [210/250], Avg Loss: 1.8878\n",
            "Epoch [7/10], Step [220/250], Avg Loss: 1.8878\n",
            "Epoch [7/10], Step [230/250], Avg Loss: 1.8887\n",
            "Epoch [7/10], Step [240/250], Avg Loss: 1.8866\n",
            "Epoch [7/10], Step [250/250], Avg Loss: 1.8874\n",
            "Train:\n",
            "Accuracy: 0.326375 Loss: 1.8873804006576538\n",
            "Validation:\n",
            "Accuracy: 0.32366183091545775 Loss: 1.877536445095755 \n",
            "\n",
            "Epoch [8/10], Step [10/250], Avg Loss: 1.8508\n",
            "Epoch [8/10], Step [20/250], Avg Loss: 1.8598\n",
            "Epoch [8/10], Step [30/250], Avg Loss: 1.8735\n",
            "Epoch [8/10], Step [40/250], Avg Loss: 1.8710\n",
            "Epoch [8/10], Step [50/250], Avg Loss: 1.8648\n",
            "Epoch [8/10], Step [60/250], Avg Loss: 1.8547\n",
            "Epoch [8/10], Step [70/250], Avg Loss: 1.8531\n",
            "Epoch [8/10], Step [80/250], Avg Loss: 1.8401\n",
            "Epoch [8/10], Step [90/250], Avg Loss: 1.8486\n",
            "Epoch [8/10], Step [100/250], Avg Loss: 1.8559\n",
            "Epoch [8/10], Step [110/250], Avg Loss: 1.8559\n",
            "Epoch [8/10], Step [120/250], Avg Loss: 1.8590\n",
            "Epoch [8/10], Step [130/250], Avg Loss: 1.8637\n",
            "Epoch [8/10], Step [140/250], Avg Loss: 1.8716\n",
            "Epoch [8/10], Step [150/250], Avg Loss: 1.8721\n",
            "Epoch [8/10], Step [160/250], Avg Loss: 1.8699\n",
            "Epoch [8/10], Step [170/250], Avg Loss: 1.8686\n",
            "Epoch [8/10], Step [180/250], Avg Loss: 1.8689\n",
            "Epoch [8/10], Step [190/250], Avg Loss: 1.8702\n",
            "Epoch [8/10], Step [200/250], Avg Loss: 1.8709\n",
            "Epoch [8/10], Step [210/250], Avg Loss: 1.8681\n",
            "Epoch [8/10], Step [220/250], Avg Loss: 1.8617\n",
            "Epoch [8/10], Step [230/250], Avg Loss: 1.8591\n",
            "Epoch [8/10], Step [240/250], Avg Loss: 1.8612\n",
            "Epoch [8/10], Step [250/250], Avg Loss: 1.8588\n",
            "Train:\n",
            "Accuracy: 0.3275 Loss: 1.8588447213172912\n",
            "Validation:\n",
            "Accuracy: 0.3586793396698349 Loss: 1.8094281302266504 \n",
            "\n",
            "Epoch [9/10], Step [10/250], Avg Loss: 1.8851\n",
            "Epoch [9/10], Step [20/250], Avg Loss: 1.8586\n",
            "Epoch [9/10], Step [30/250], Avg Loss: 1.8586\n",
            "Epoch [9/10], Step [40/250], Avg Loss: 1.8328\n",
            "Epoch [9/10], Step [50/250], Avg Loss: 1.8348\n",
            "Epoch [9/10], Step [60/250], Avg Loss: 1.8402\n",
            "Epoch [9/10], Step [70/250], Avg Loss: 1.8269\n",
            "Epoch [9/10], Step [80/250], Avg Loss: 1.8367\n",
            "Epoch [9/10], Step [90/250], Avg Loss: 1.8405\n",
            "Epoch [9/10], Step [100/250], Avg Loss: 1.8415\n",
            "Epoch [9/10], Step [110/250], Avg Loss: 1.8421\n",
            "Epoch [9/10], Step [120/250], Avg Loss: 1.8389\n",
            "Epoch [9/10], Step [130/250], Avg Loss: 1.8350\n",
            "Epoch [9/10], Step [140/250], Avg Loss: 1.8342\n",
            "Epoch [9/10], Step [150/250], Avg Loss: 1.8397\n",
            "Epoch [9/10], Step [160/250], Avg Loss: 1.8410\n",
            "Epoch [9/10], Step [170/250], Avg Loss: 1.8411\n",
            "Epoch [9/10], Step [180/250], Avg Loss: 1.8406\n",
            "Epoch [9/10], Step [190/250], Avg Loss: 1.8363\n",
            "Epoch [9/10], Step [200/250], Avg Loss: 1.8385\n",
            "Epoch [9/10], Step [210/250], Avg Loss: 1.8344\n",
            "Epoch [9/10], Step [220/250], Avg Loss: 1.8356\n",
            "Epoch [9/10], Step [230/250], Avg Loss: 1.8381\n",
            "Epoch [9/10], Step [240/250], Avg Loss: 1.8376\n",
            "Epoch [9/10], Step [250/250], Avg Loss: 1.8356\n",
            "Train:\n",
            "Accuracy: 0.349 Loss: 1.8355605235099792\n",
            "Validation:\n",
            "Accuracy: 0.3706853426713357 Loss: 1.8114647300557054 \n",
            "\n",
            "Epoch [10/10], Step [10/250], Avg Loss: 1.7676\n",
            "Epoch [10/10], Step [20/250], Avg Loss: 1.7617\n",
            "Epoch [10/10], Step [30/250], Avg Loss: 1.7549\n",
            "Epoch [10/10], Step [40/250], Avg Loss: 1.7655\n",
            "Epoch [10/10], Step [50/250], Avg Loss: 1.7722\n",
            "Epoch [10/10], Step [60/250], Avg Loss: 1.7859\n",
            "Epoch [10/10], Step [70/250], Avg Loss: 1.7929\n",
            "Epoch [10/10], Step [80/250], Avg Loss: 1.7925\n",
            "Epoch [10/10], Step [90/250], Avg Loss: 1.7855\n",
            "Epoch [10/10], Step [100/250], Avg Loss: 1.7894\n",
            "Epoch [10/10], Step [110/250], Avg Loss: 1.7955\n",
            "Epoch [10/10], Step [120/250], Avg Loss: 1.7975\n",
            "Epoch [10/10], Step [130/250], Avg Loss: 1.7994\n",
            "Epoch [10/10], Step [140/250], Avg Loss: 1.8058\n",
            "Epoch [10/10], Step [150/250], Avg Loss: 1.8080\n",
            "Epoch [10/10], Step [160/250], Avg Loss: 1.8061\n",
            "Epoch [10/10], Step [170/250], Avg Loss: 1.8071\n",
            "Epoch [10/10], Step [180/250], Avg Loss: 1.8109\n",
            "Epoch [10/10], Step [190/250], Avg Loss: 1.8109\n",
            "Epoch [10/10], Step [200/250], Avg Loss: 1.8118\n",
            "Epoch [10/10], Step [210/250], Avg Loss: 1.8092\n",
            "Epoch [10/10], Step [220/250], Avg Loss: 1.8067\n",
            "Epoch [10/10], Step [230/250], Avg Loss: 1.8073\n",
            "Epoch [10/10], Step [240/250], Avg Loss: 1.8057\n",
            "Epoch [10/10], Step [250/250], Avg Loss: 1.8042\n",
            "Train:\n",
            "Accuracy: 0.351625 Loss: 1.8041812739372254\n",
            "Validation:\n",
            "Accuracy: 0.37868934467233617 Loss: 1.7863905153136184 \n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhelpful-donkey-17\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized/runs/goi9gewx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240409_112920-goi9gewx/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -s 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTIaA0qhl5U5",
        "outputId": "bd45f6d6-4895-4284-84fa-636cb7850fed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m017\u001b[0m (\u001b[33marun_cs23m017\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240409_120107-9ytzjcem\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchocolate-forest-18\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized/runs/9ytzjcem\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch [1/10], Step [10/250], Avg Loss: 2.7388\n",
            "Epoch [1/10], Step [20/250], Avg Loss: 2.4131\n",
            "Epoch [1/10], Step [30/250], Avg Loss: 2.1973\n",
            "Epoch [1/10], Step [40/250], Avg Loss: 2.0590\n",
            "Epoch [1/10], Step [50/250], Avg Loss: 1.9472\n",
            "Epoch [1/10], Step [60/250], Avg Loss: 1.8561\n",
            "Epoch [1/10], Step [70/250], Avg Loss: 1.7825\n",
            "Epoch [1/10], Step [80/250], Avg Loss: 1.7453\n",
            "Epoch [1/10], Step [90/250], Avg Loss: 1.7036\n",
            "Epoch [1/10], Step [100/250], Avg Loss: 1.6732\n",
            "Epoch [1/10], Step [110/250], Avg Loss: 1.6493\n",
            "Epoch [1/10], Step [120/250], Avg Loss: 1.6298\n",
            "Epoch [1/10], Step [130/250], Avg Loss: 1.6143\n",
            "Epoch [1/10], Step [140/250], Avg Loss: 1.5895\n",
            "Epoch [1/10], Step [150/250], Avg Loss: 1.5664\n",
            "Epoch [1/10], Step [160/250], Avg Loss: 1.5536\n",
            "Epoch [1/10], Step [170/250], Avg Loss: 1.5406\n",
            "Epoch [1/10], Step [180/250], Avg Loss: 1.5341\n",
            "Epoch [1/10], Step [190/250], Avg Loss: 1.5272\n",
            "Epoch [1/10], Step [200/250], Avg Loss: 1.5244\n",
            "Epoch [1/10], Step [210/250], Avg Loss: 1.5132\n",
            "Epoch [1/10], Step [220/250], Avg Loss: 1.5047\n",
            "Epoch [1/10], Step [230/250], Avg Loss: 1.5023\n",
            "Epoch [1/10], Step [240/250], Avg Loss: 1.5023\n",
            "Epoch [1/10], Step [250/250], Avg Loss: 1.4973\n",
            "Train:\n",
            "Accuracy: 0.488875 Loss: 1.4972675657272339\n",
            "Validation:\n",
            "Accuracy: 0.51975987993997 Loss: 1.35923619691344 \n",
            "\n",
            "Epoch [2/10], Step [10/250], Avg Loss: 1.2207\n",
            "Epoch [2/10], Step [20/250], Avg Loss: 1.2942\n",
            "Epoch [2/10], Step [30/250], Avg Loss: 1.2893\n",
            "Epoch [2/10], Step [40/250], Avg Loss: 1.2860\n",
            "Epoch [2/10], Step [50/250], Avg Loss: 1.2762\n",
            "Epoch [2/10], Step [60/250], Avg Loss: 1.2676\n",
            "Epoch [2/10], Step [70/250], Avg Loss: 1.2600\n",
            "Epoch [2/10], Step [80/250], Avg Loss: 1.2536\n",
            "Epoch [2/10], Step [90/250], Avg Loss: 1.2511\n",
            "Epoch [2/10], Step [100/250], Avg Loss: 1.2489\n",
            "Epoch [2/10], Step [110/250], Avg Loss: 1.2524\n",
            "Epoch [2/10], Step [120/250], Avg Loss: 1.2492\n",
            "Epoch [2/10], Step [130/250], Avg Loss: 1.2537\n",
            "Epoch [2/10], Step [140/250], Avg Loss: 1.2554\n",
            "Epoch [2/10], Step [150/250], Avg Loss: 1.2503\n",
            "Epoch [2/10], Step [160/250], Avg Loss: 1.2407\n",
            "Epoch [2/10], Step [170/250], Avg Loss: 1.2383\n",
            "Epoch [2/10], Step [180/250], Avg Loss: 1.2333\n",
            "Epoch [2/10], Step [190/250], Avg Loss: 1.2310\n",
            "Epoch [2/10], Step [200/250], Avg Loss: 1.2302\n",
            "Epoch [2/10], Step [210/250], Avg Loss: 1.2323\n",
            "Epoch [2/10], Step [220/250], Avg Loss: 1.2324\n",
            "Epoch [2/10], Step [230/250], Avg Loss: 1.2337\n",
            "Epoch [2/10], Step [240/250], Avg Loss: 1.2395\n",
            "Epoch [2/10], Step [250/250], Avg Loss: 1.2404\n",
            "Train:\n",
            "Accuracy: 0.572875 Loss: 1.2403511953353883\n",
            "Validation:\n",
            "Accuracy: 0.5642821410705353 Loss: 1.2973802154692726 \n",
            "\n",
            "Epoch [3/10], Step [10/250], Avg Loss: 1.1890\n",
            "Epoch [3/10], Step [20/250], Avg Loss: 1.1711\n",
            "Epoch [3/10], Step [30/250], Avg Loss: 1.2120\n",
            "Epoch [3/10], Step [40/250], Avg Loss: 1.2268\n",
            "Epoch [3/10], Step [50/250], Avg Loss: 1.2267\n",
            "Epoch [3/10], Step [60/250], Avg Loss: 1.2204\n",
            "Epoch [3/10], Step [70/250], Avg Loss: 1.2126\n",
            "Epoch [3/10], Step [80/250], Avg Loss: 1.2084\n",
            "Epoch [3/10], Step [90/250], Avg Loss: 1.2104\n",
            "Epoch [3/10], Step [100/250], Avg Loss: 1.2169\n",
            "Epoch [3/10], Step [110/250], Avg Loss: 1.2116\n",
            "Epoch [3/10], Step [120/250], Avg Loss: 1.2116\n",
            "Epoch [3/10], Step [130/250], Avg Loss: 1.2108\n",
            "Epoch [3/10], Step [140/250], Avg Loss: 1.1984\n",
            "Epoch [3/10], Step [150/250], Avg Loss: 1.1971\n",
            "Epoch [3/10], Step [160/250], Avg Loss: 1.1986\n",
            "Epoch [3/10], Step [170/250], Avg Loss: 1.1989\n",
            "Epoch [3/10], Step [180/250], Avg Loss: 1.1926\n",
            "Epoch [3/10], Step [190/250], Avg Loss: 1.1971\n",
            "Epoch [3/10], Step [200/250], Avg Loss: 1.1937\n",
            "Epoch [3/10], Step [210/250], Avg Loss: 1.1946\n",
            "Epoch [3/10], Step [220/250], Avg Loss: 1.1940\n",
            "Epoch [3/10], Step [230/250], Avg Loss: 1.1953\n",
            "Epoch [3/10], Step [240/250], Avg Loss: 1.1942\n",
            "Epoch [3/10], Step [250/250], Avg Loss: 1.1900\n",
            "Train:\n",
            "Accuracy: 0.59175 Loss: 1.1899794681072235\n",
            "Validation:\n",
            "Accuracy: 0.5477738869434717 Loss: 1.3385123320971208 \n",
            "\n",
            "Epoch [4/10], Step [10/250], Avg Loss: 1.2446\n",
            "Epoch [4/10], Step [20/250], Avg Loss: 1.2288\n",
            "Epoch [4/10], Step [30/250], Avg Loss: 1.2098\n",
            "Epoch [4/10], Step [40/250], Avg Loss: 1.2037\n",
            "Epoch [4/10], Step [50/250], Avg Loss: 1.2114\n",
            "Epoch [4/10], Step [60/250], Avg Loss: 1.2089\n",
            "Epoch [4/10], Step [70/250], Avg Loss: 1.2146\n",
            "Epoch [4/10], Step [80/250], Avg Loss: 1.2217\n",
            "Epoch [4/10], Step [90/250], Avg Loss: 1.2185\n",
            "Epoch [4/10], Step [100/250], Avg Loss: 1.2074\n",
            "Epoch [4/10], Step [110/250], Avg Loss: 1.1974\n",
            "Epoch [4/10], Step [120/250], Avg Loss: 1.1983\n",
            "Epoch [4/10], Step [130/250], Avg Loss: 1.2003\n",
            "Epoch [4/10], Step [140/250], Avg Loss: 1.1984\n",
            "Epoch [4/10], Step [150/250], Avg Loss: 1.1957\n",
            "Epoch [4/10], Step [160/250], Avg Loss: 1.1918\n",
            "Epoch [4/10], Step [170/250], Avg Loss: 1.1918\n",
            "Epoch [4/10], Step [180/250], Avg Loss: 1.1990\n",
            "Epoch [4/10], Step [190/250], Avg Loss: 1.1958\n",
            "Epoch [4/10], Step [200/250], Avg Loss: 1.1975\n",
            "Epoch [4/10], Step [210/250], Avg Loss: 1.2001\n",
            "Epoch [4/10], Step [220/250], Avg Loss: 1.1997\n",
            "Epoch [4/10], Step [230/250], Avg Loss: 1.1983\n",
            "Epoch [4/10], Step [240/250], Avg Loss: 1.1943\n",
            "Epoch [4/10], Step [250/250], Avg Loss: 1.1929\n",
            "Train:\n",
            "Accuracy: 0.592875 Loss: 1.1929197058677674\n",
            "Validation:\n",
            "Accuracy: 0.5562781390695347 Loss: 1.2884362822118074 \n",
            "\n",
            "Epoch [5/10], Step [10/250], Avg Loss: 1.0394\n",
            "Epoch [5/10], Step [20/250], Avg Loss: 1.0505\n",
            "Epoch [5/10], Step [30/250], Avg Loss: 1.1176\n",
            "Epoch [5/10], Step [40/250], Avg Loss: 1.1105\n",
            "Epoch [5/10], Step [50/250], Avg Loss: 1.1165\n",
            "Epoch [5/10], Step [60/250], Avg Loss: 1.1124\n",
            "Epoch [5/10], Step [70/250], Avg Loss: 1.1091\n",
            "Epoch [5/10], Step [80/250], Avg Loss: 1.1102\n",
            "Epoch [5/10], Step [90/250], Avg Loss: 1.1210\n",
            "Epoch [5/10], Step [100/250], Avg Loss: 1.1194\n",
            "Epoch [5/10], Step [110/250], Avg Loss: 1.1243\n",
            "Epoch [5/10], Step [120/250], Avg Loss: 1.1240\n",
            "Epoch [5/10], Step [130/250], Avg Loss: 1.1269\n",
            "Epoch [5/10], Step [140/250], Avg Loss: 1.1354\n",
            "Epoch [5/10], Step [150/250], Avg Loss: 1.1381\n",
            "Epoch [5/10], Step [160/250], Avg Loss: 1.1378\n",
            "Epoch [5/10], Step [170/250], Avg Loss: 1.1331\n",
            "Epoch [5/10], Step [180/250], Avg Loss: 1.1371\n",
            "Epoch [5/10], Step [190/250], Avg Loss: 1.1364\n",
            "Epoch [5/10], Step [200/250], Avg Loss: 1.1380\n",
            "Epoch [5/10], Step [210/250], Avg Loss: 1.1395\n",
            "Epoch [5/10], Step [220/250], Avg Loss: 1.1505\n",
            "Epoch [5/10], Step [230/250], Avg Loss: 1.1517\n",
            "Epoch [5/10], Step [240/250], Avg Loss: 1.1558\n",
            "Epoch [5/10], Step [250/250], Avg Loss: 1.1578\n",
            "Train:\n",
            "Accuracy: 0.6035 Loss: 1.1578408694267273\n",
            "Validation:\n",
            "Accuracy: 0.5467733866933466 Loss: 1.2794021795605826 \n",
            "\n",
            "Epoch [6/10], Step [10/250], Avg Loss: 1.0027\n",
            "Epoch [6/10], Step [20/250], Avg Loss: 1.0586\n",
            "Epoch [6/10], Step [30/250], Avg Loss: 1.0892\n",
            "Epoch [6/10], Step [40/250], Avg Loss: 1.0980\n",
            "Epoch [6/10], Step [50/250], Avg Loss: 1.0924\n",
            "Epoch [6/10], Step [60/250], Avg Loss: 1.0783\n",
            "Epoch [6/10], Step [70/250], Avg Loss: 1.0942\n",
            "Epoch [6/10], Step [80/250], Avg Loss: 1.0996\n",
            "Epoch [6/10], Step [90/250], Avg Loss: 1.1186\n",
            "Epoch [6/10], Step [100/250], Avg Loss: 1.1217\n",
            "Epoch [6/10], Step [110/250], Avg Loss: 1.1129\n",
            "Epoch [6/10], Step [120/250], Avg Loss: 1.1156\n",
            "Epoch [6/10], Step [130/250], Avg Loss: 1.1252\n",
            "Epoch [6/10], Step [140/250], Avg Loss: 1.1268\n",
            "Epoch [6/10], Step [150/250], Avg Loss: 1.1297\n",
            "Epoch [6/10], Step [160/250], Avg Loss: 1.1249\n",
            "Epoch [6/10], Step [170/250], Avg Loss: 1.1294\n",
            "Epoch [6/10], Step [180/250], Avg Loss: 1.1392\n",
            "Epoch [6/10], Step [190/250], Avg Loss: 1.1367\n",
            "Epoch [6/10], Step [200/250], Avg Loss: 1.1366\n",
            "Epoch [6/10], Step [210/250], Avg Loss: 1.1433\n",
            "Epoch [6/10], Step [220/250], Avg Loss: 1.1439\n",
            "Epoch [6/10], Step [230/250], Avg Loss: 1.1460\n",
            "Epoch [6/10], Step [240/250], Avg Loss: 1.1509\n",
            "Epoch [6/10], Step [250/250], Avg Loss: 1.1512\n",
            "Train:\n",
            "Accuracy: 0.6075 Loss: 1.1511894426345826\n",
            "Validation:\n",
            "Accuracy: 0.5742871435717859 Loss: 1.261755599863473 \n",
            "\n",
            "Epoch [7/10], Step [10/250], Avg Loss: 1.1548\n",
            "Epoch [7/10], Step [20/250], Avg Loss: 1.1216\n",
            "Epoch [7/10], Step [30/250], Avg Loss: 1.1643\n",
            "Epoch [7/10], Step [40/250], Avg Loss: 1.1593\n",
            "Epoch [7/10], Step [50/250], Avg Loss: 1.1363\n",
            "Epoch [7/10], Step [60/250], Avg Loss: 1.1401\n",
            "Epoch [7/10], Step [70/250], Avg Loss: 1.1348\n",
            "Epoch [7/10], Step [80/250], Avg Loss: 1.1271\n",
            "Epoch [7/10], Step [90/250], Avg Loss: 1.1374\n",
            "Epoch [7/10], Step [100/250], Avg Loss: 1.1294\n",
            "Epoch [7/10], Step [110/250], Avg Loss: 1.1246\n",
            "Epoch [7/10], Step [120/250], Avg Loss: 1.1245\n",
            "Epoch [7/10], Step [130/250], Avg Loss: 1.1196\n",
            "Epoch [7/10], Step [140/250], Avg Loss: 1.1184\n",
            "Epoch [7/10], Step [150/250], Avg Loss: 1.1217\n",
            "Epoch [7/10], Step [160/250], Avg Loss: 1.1226\n",
            "Epoch [7/10], Step [170/250], Avg Loss: 1.1241\n",
            "Epoch [7/10], Step [180/250], Avg Loss: 1.1226\n",
            "Epoch [7/10], Step [190/250], Avg Loss: 1.1257\n",
            "Epoch [7/10], Step [200/250], Avg Loss: 1.1215\n",
            "Epoch [7/10], Step [210/250], Avg Loss: 1.1178\n",
            "Epoch [7/10], Step [220/250], Avg Loss: 1.1178\n",
            "Epoch [7/10], Step [230/250], Avg Loss: 1.1230\n",
            "Epoch [7/10], Step [240/250], Avg Loss: 1.1252\n",
            "Epoch [7/10], Step [250/250], Avg Loss: 1.1262\n",
            "Train:\n",
            "Accuracy: 0.617875 Loss: 1.1261666343212127\n",
            "Validation:\n",
            "Accuracy: 0.5717858929464732 Loss: 1.322640057502239 \n",
            "\n",
            "Epoch [8/10], Step [10/250], Avg Loss: 1.0552\n",
            "Epoch [8/10], Step [20/250], Avg Loss: 1.1322\n",
            "Epoch [8/10], Step [30/250], Avg Loss: 1.1337\n",
            "Epoch [8/10], Step [40/250], Avg Loss: 1.1269\n",
            "Epoch [8/10], Step [50/250], Avg Loss: 1.1258\n",
            "Epoch [8/10], Step [60/250], Avg Loss: 1.1159\n",
            "Epoch [8/10], Step [70/250], Avg Loss: 1.1097\n",
            "Epoch [8/10], Step [80/250], Avg Loss: 1.1164\n",
            "Epoch [8/10], Step [90/250], Avg Loss: 1.1161\n",
            "Epoch [8/10], Step [100/250], Avg Loss: 1.1225\n",
            "Epoch [8/10], Step [110/250], Avg Loss: 1.1187\n",
            "Epoch [8/10], Step [120/250], Avg Loss: 1.1186\n",
            "Epoch [8/10], Step [130/250], Avg Loss: 1.1275\n",
            "Epoch [8/10], Step [140/250], Avg Loss: 1.1260\n",
            "Epoch [8/10], Step [150/250], Avg Loss: 1.1242\n",
            "Epoch [8/10], Step [160/250], Avg Loss: 1.1289\n",
            "Epoch [8/10], Step [170/250], Avg Loss: 1.1245\n",
            "Epoch [8/10], Step [180/250], Avg Loss: 1.1251\n",
            "Epoch [8/10], Step [190/250], Avg Loss: 1.1325\n",
            "Epoch [8/10], Step [200/250], Avg Loss: 1.1372\n",
            "Epoch [8/10], Step [210/250], Avg Loss: 1.1402\n",
            "Epoch [8/10], Step [220/250], Avg Loss: 1.1382\n",
            "Epoch [8/10], Step [230/250], Avg Loss: 1.1425\n",
            "Epoch [8/10], Step [240/250], Avg Loss: 1.1436\n",
            "Epoch [8/10], Step [250/250], Avg Loss: 1.1468\n",
            "Train:\n",
            "Accuracy: 0.611625 Loss: 1.1468341765403747\n",
            "Validation:\n",
            "Accuracy: 0.5537768884442221 Loss: 1.301133817079486 \n",
            "\n",
            "Epoch [9/10], Step [10/250], Avg Loss: 1.1746\n",
            "Epoch [9/10], Step [20/250], Avg Loss: 1.1158\n",
            "Epoch [9/10], Step [30/250], Avg Loss: 1.1489\n",
            "Epoch [9/10], Step [40/250], Avg Loss: 1.1365\n",
            "Epoch [9/10], Step [50/250], Avg Loss: 1.1401\n",
            "Epoch [9/10], Step [60/250], Avg Loss: 1.1518\n",
            "Epoch [9/10], Step [70/250], Avg Loss: 1.1408\n",
            "Epoch [9/10], Step [80/250], Avg Loss: 1.1237\n",
            "Epoch [9/10], Step [90/250], Avg Loss: 1.1206\n",
            "Epoch [9/10], Step [100/250], Avg Loss: 1.1192\n",
            "Epoch [9/10], Step [110/250], Avg Loss: 1.1038\n",
            "Epoch [9/10], Step [120/250], Avg Loss: 1.1080\n",
            "Epoch [9/10], Step [130/250], Avg Loss: 1.1155\n",
            "Epoch [9/10], Step [140/250], Avg Loss: 1.1164\n",
            "Epoch [9/10], Step [150/250], Avg Loss: 1.1155\n",
            "Epoch [9/10], Step [160/250], Avg Loss: 1.1209\n",
            "Epoch [9/10], Step [170/250], Avg Loss: 1.1178\n",
            "Epoch [9/10], Step [180/250], Avg Loss: 1.1106\n",
            "Epoch [9/10], Step [190/250], Avg Loss: 1.1170\n",
            "Epoch [9/10], Step [200/250], Avg Loss: 1.1180\n",
            "Epoch [9/10], Step [210/250], Avg Loss: 1.1189\n",
            "Epoch [9/10], Step [220/250], Avg Loss: 1.1162\n",
            "Epoch [9/10], Step [230/250], Avg Loss: 1.1154\n",
            "Epoch [9/10], Step [240/250], Avg Loss: 1.1185\n",
            "Epoch [9/10], Step [250/250], Avg Loss: 1.1173\n",
            "Train:\n",
            "Accuracy: 0.6215 Loss: 1.117331713438034\n",
            "Validation:\n",
            "Accuracy: 0.5902951475737869 Loss: 1.2097689504322855 \n",
            "\n",
            "Epoch [10/10], Step [10/250], Avg Loss: 1.0352\n",
            "Epoch [10/10], Step [20/250], Avg Loss: 1.0673\n",
            "Epoch [10/10], Step [30/250], Avg Loss: 1.0987\n",
            "Epoch [10/10], Step [40/250], Avg Loss: 1.0955\n",
            "Epoch [10/10], Step [50/250], Avg Loss: 1.0958\n",
            "Epoch [10/10], Step [60/250], Avg Loss: 1.0951\n",
            "Epoch [10/10], Step [70/250], Avg Loss: 1.0911\n",
            "Epoch [10/10], Step [80/250], Avg Loss: 1.0845\n",
            "Epoch [10/10], Step [90/250], Avg Loss: 1.0853\n",
            "Epoch [10/10], Step [100/250], Avg Loss: 1.0835\n",
            "Epoch [10/10], Step [110/250], Avg Loss: 1.0830\n",
            "Epoch [10/10], Step [120/250], Avg Loss: 1.0796\n",
            "Epoch [10/10], Step [130/250], Avg Loss: 1.0818\n",
            "Epoch [10/10], Step [140/250], Avg Loss: 1.0821\n",
            "Epoch [10/10], Step [150/250], Avg Loss: 1.0824\n",
            "Epoch [10/10], Step [160/250], Avg Loss: 1.0875\n",
            "Epoch [10/10], Step [170/250], Avg Loss: 1.0884\n",
            "Epoch [10/10], Step [180/250], Avg Loss: 1.0872\n",
            "Epoch [10/10], Step [190/250], Avg Loss: 1.0881\n",
            "Epoch [10/10], Step [200/250], Avg Loss: 1.0894\n",
            "Epoch [10/10], Step [210/250], Avg Loss: 1.0894\n",
            "Epoch [10/10], Step [220/250], Avg Loss: 1.0901\n",
            "Epoch [10/10], Step [230/250], Avg Loss: 1.0934\n",
            "Epoch [10/10], Step [240/250], Avg Loss: 1.0905\n",
            "Epoch [10/10], Step [250/250], Avg Loss: 1.0938\n",
            "Train:\n",
            "Accuracy: 0.62775 Loss: 1.0938369708061217\n",
            "Validation:\n",
            "Accuracy: 0.5892946473236619 Loss: 1.2261071870063411 \n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mchocolate-forest-18\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized/runs/9ytzjcem\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/arun_cs23m017/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240409_120107-9ytzjcem/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}